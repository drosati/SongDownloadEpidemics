## Name: PlotAndParmsPdf
## Author: Dora Rosati
## Created: April 24, 2015
## Modified: July 8, 2015 (DR - edited the song data frame so that the names of
##                         3 problem genres are fixed and so that $ and & are
##                         replaced with escaped characters)
## Modified: August 3, 2015 (DR - added code to aggregate the beginning of songs
##                           missing beginnings at a finer timescale than the rest
##                           of the time series)
## Modified: Oct. 14, 2015 (DR - fixed issue where cc was not properly asisgned
##                          for songs with missing beginnings)
## Modified: Nov. 3, 2015 (DR - added options to plot chunks and sourced files
##                         for min.R0 and max.rel.fit)
## Modified: Nov. 9, 2015 (DR - added songstoexcludedf, pre.peak.fit, chunk fit
##                         vectors and cumulative fit vectors and put these in
##                         final data frame)
## Modified: Nov. 11, 2015 (DR - added code to determine where to cut time series,
##                          to split time series into 2 or 4 chunks depending on
##                          location of peak, and to calculate goodness of fit in
##                          each chunk and store this; also code for cumulative sum
##                          at each point, fit for entire chunk before peak, code
##                          to plot lines where curve is cut; also code to store
##                          time of each cut)
## Modified: Nov. 12, 2015 (DR - changed NULLs to NAs; added code to check if there
##                          is enough before the peak to be split in half)
## Modified: March 15, 2016 (DR - added check for duplicate files for songs in
##                           datafilename vector)
## Modified: March 16, 2016 (DR - added check for duplicate files that differ only
##                           by capitalization in their datafilename)
## Modified: April 26, 2016 (DR - got rid of world option)
## Modified: July 12, 2016 (DR - changed fit measure to root mean square (NOTE THIS
##                          HASN'T BEEN DONE FOR CHUNKS OF FIT!), added bit to skip
##                          things if the fit is NULL (because at the moment
##                          we're looking at a list of fits that is incomplete)
## Modified: July 13, 2016 (DR - changed where .RData file is loaded so that it is
##                          only loaded once per country)
## Modified: July 17, 2016 (DR - added chunk to script to get rid of songs without
##                          fits based on information from the saved timescale
##                          vector)
## Modified: July 18, 2016 (DR - made the elimination of songs without fits an
##                          option that can be set, changed where in script vectors
##                          that store parameters are created so that they will be
##                          the proper length)
## Modified: July 21, 2016 (DR - added commented out bit of code that was used to
##                          cut groups of 200 songs into manageable chunks)
## Modified: Aug 10, 2019  (DR - added a variable to store coef(m1) so that the name
##                          of the fourth parameter in this object could be corrected
##                          - was "logit.i", but trans.pars expects "logit.i0)
## Modified: 
##
## Purpose: To load .RData files generated by PlotAndParmsRdaOnly.R and use
## them to create plots of download time series with their fitted curves for
## the songs listed. Will also extract parameters and measures of goodness of
## fit and store these in an .RData file and a .tsv file.
##
## To Run: Set appropriate working directory in R and type
## 'source("PlotAndParmsPdf.R")' OR with:
##      time R CMD BATCH --vanilla PlotAndParmsPdf.R &


################################################################
## READ IN SONG DATA ###########################################
################################################################

# Clear workspace
rm(list=ls())

# set number of warnings printed to be much higher so that they will all
# be printed to the .Rout file at the end
options(nwarnings=100000)

# Load a few necessary packages:
library("rJava")
library("stats4")
library("nloptr")
library("plyr") # needed by dplyr
library("dplyr") # for data manipulation
library("lubridate") # for dealing with dates
library("timeDate") # also for dealing with dates
library("fitsir") # requires bbmle and deSolve
library("bbmle")
library("deSolve")

# Load function to aggregate download data at appropriate time scale
# Note: this script loads plyr, dplyr, mgcv and lubridate
source("../timeaggfuns.R")

## Load file of songs to exclude eventually
songstoexcludedf <- read.delim("../ExcludedSongsGB.tsv",quote="")
names(songstoexcludedf) <- "title"

# Load file with information on songs that you want to extract
# download data for (as a data frame)
# NOTE: THE USER SPECIFIES THIS FILENAME HERE!
songlistdf <- read.delim("top1000GBsongs.tsv",quote="")
names(songlistdf) <- c("count","title","artist","genre")

# Create a column of filenames for each song by removing all spaces
# and punctuation from the song title and artist columns and putting
# them together
songtitle <- with(songlistdf,gsub(" ","",title))
songtitle <- gsub("[[:punct:]]","",songtitle)
artistname <- with(songlistdf,gsub(" ","",artist))
artistname <- gsub("[[:punct:]]","",artistname)
datafilename <- paste(songtitle,artistname,sep="_")

stopifnot(length(datafilename)==length(songlistdf$title))

## check for duplicates in datafilename vector
## this might slow things down a lot, there is probably a better way to do it...
for (i in 2:length(datafilename)) { # start from 2 because otherwise the first song will be compared with itself
  if (is.element(tolower(datafilename[i]),tolower(datafilename[1:(i-1)]))) { # check if the file name exists with any capitalization differences because these are not enough to distinguish
    datafilename[i] <- sprintf("%s2",datafilename[i])
  }
}

## Add escape characters for apostrophes. Previously done so
## that they wouldn't mess up sql queries when they were executed
## (this replaces ' with \'), now important for R manipulation, LaTeX, etc.
songlistdf <- songlistdf %>%
  mutate(artist=gsub("'","\\\\'",songlistdf$artist),
         title=gsub("'","\\\\'",songlistdf$title))

## Also add escape characters for the $ and & characters, since these will
## mess up Latex
songlistdf <- songlistdf %>%
    mutate(artist=gsub("[&]","\\\\&",songlistdf$artist),
           title=gsub("[&]","\\\\&",songlistdf$title))
songlistdf <- songlistdf %>%
    mutate(artist=gsub("[$]","\\\\$",songlistdf$artist),
           title=gsub("[$]","\\\\$",songlistdf$title))

## and finally fix the titles of three genres that will create problems for latex
## switch R 'n' B to R&B
levels(songlistdf$genre)[levels(songlistdf$genre)=="Soul/R 'n' B/Funk"] <- "Soul/R\\&B/Funk"

## fix the & in Indipop & Remix
levels(songlistdf$genre)[levels(songlistdf$genre)=="Indipop & Remix"] <- "Indipop \\& Remix"

## fix the _ in rock_and_pop
levels(songlistdf$genre)[levels(songlistdf$genre)=="rock_and_pop"] <- "rock\\_and\\_pop"


################################################################
## SPECIFY QUERY PARAMETERS ####################################
################################################################

# specify whether you want data for a set of specific countries
# or the whole world
some.countries <- TRUE
worldwide <- FALSE
stopifnot(some.countries!=worldwide)

# if specific countries, load a list of countries that we want to look at
if (some.countries) {
  countrylist <- as.vector(t(read.table("../countrylist.tsv",header=TRUE)))
}

## some timescale options
ymd.timescale <- c("month","week","3.5 day","day")
hms.timescale <- c("12 hour","6 hour","3 hour","1 hour","30 min","15 min")

## set minimum R0 to for excluding songs to check fit chunks
source("minR0calc.R") # this will end up creating a parameter min.R0,
                      # which is the lowest possible R0

## set various parameter limits by sourcing a file with settings -- need max.rel.fit in this case
## can reference a different file for (example) users with 5-5000 downloads
source("../ParameterSettings.R")

## decide whether we want to split the fits into chunks at all
do.chunks <- FALSE
## decide whether we want to plot vertical lines showing where we cut
## the time series into chunks to look at goodness of fit further
plot.chunks <- FALSE
## decide if we want a fit for the entire bit before the peak too
pre.peak.fit <- TRUE
## specify whether we are checking for songs in the list that don't have a proper fit
missing.fits <- FALSE

##dolphin remove
b <- 0


################################################################
## DEFINE FINAL SIZE FUNCTION ##################################
################################################################

require(emdbook)
Z <- function(R) {
  1+1/R*lambertW(-R*exp(-R))
}


################################################################
## PLOT DATA AND SIR FITS ######################################
################################################################

## FIXME: debugging
options(error=recover)

# Code to plot data if we are looking at downloads in a specific
# list of countries
if (some.countries) {
  for (icountry in 1:length(countrylist)) {
    country <- countrylist[icountry]

    ## load .RData file
    ## NOTE: The user picks which RData file to use here
    rdafilename <- paste(sprintf("TopSongs%sFitsAndTimescales_Accents.RData",country))
    load(rdafilename)

    ## start from the songs that this job started on...
    #songlistdf <- songlistdf[801:nrow(songlistdf),]
    #datafilename <- datafilename[801:length(datafilename)]

    ##FIXME: this bit of code was needed to negotiate around the issues with not being able to deal with more than 83 songs at a time in the fourth group of 200 songs,
    ## and a similar issue in the second and last group of 200 songs -- could certainly have been more elegantly implemented
    #n.start <- which(songlistdf$title=="Paul")
    #browser()

    #songlistdf <- songlistdf[n.start:nrow(songlistdf),]
    #datafilename <- datafilename[n.start:length(datafilename)]
    #timescale.save <- timescale.save[n.start:length(timescale.save)]

    #browser()

    ## remove songs that have no fit by using the timescale vector -- remember to remove them from the datafilename vector!
    ## FIXME: in future we may want to save the song.fits list differently so that we can look at this instead of the timescale vector; however, this does work fine
    ## also save the songs that were not successfully fitted -- FIXME: in future code this to not print the rest of the 1000 songs, just those in the 200 we're looking at
    if (missing.fits) {
      notfitted <- songlistdf[which(timescale.save==0),]
      print(notfitted)
      save(notfitted,file="notfitted_accents.RData")
      songlistdf <- songlistdf[which(timescale.save!=0),]
      datafilename <- datafilename[which(timescale.save!=0)]
      timescale.save <- timescale.save[timescale.save!=0]
    }

    #browser()
    
    # create vectors to store various SIR parameters for each song
    R0.vec <- rep(0,nrow(songlistdf))
    gamma.vec <- rep(0,nrow(songlistdf))
    beta.vec <- rep(0,nrow(songlistdf))
    infper.vec <- rep(0,nrow(songlistdf))
    r.vec <- rep(0,nrow(songlistdf))
    N.vec <- rep(0,nrow(songlistdf))
    s0.vec <- rep(0,nrow(songlistdf))
    i0.vec <- rep(0,nrow(songlistdf))
    s0calc.vec <- rep(0,nrow(songlistdf))
    z.vec <- rep(0,nrow(songlistdf))
    rel.fit.goodness.vec <- rep(0,nrow(songlistdf))
    abs.fit.goodness.vec <- rep(0,nrow(songlistdf))
    
    ## create vectors for goodness of fit for each chunk of curve, to track cumulative sum of downloads, and to store times of cuts for each chunk
    chunk1.rel.fit.vec <- rep(NA,nrow(songlistdf))
    chunk2.rel.fit.vec <- rep(NA,nrow(songlistdf))
    chunk3.rel.fit.vec <- rep(NA,nrow(songlistdf))
    chunk4.rel.fit.vec <- rep(NA,nrow(songlistdf))
    ccs1.vec <- rep(NA,nrow(songlistdf))
    ccs2.vec <- rep(NA,nrow(songlistdf))
    ccs3.vec <- rep(NA,nrow(songlistdf))
    if (pre.peak.fit) {
      pre.peak.rel.fit.vec <- rep(NA,nrow(songlistdf))
    }
    cut1.vec <- rep(NA,nrow(songlistdf))
    cut2.vec <- rep(NA,nrow(songlistdf))
    cut3.vec <- rep(NA,nrow(songlistdf))

    for (isong in 1:nrow(songlistdf)) {
      ## set all variables for each song to be used in plots
      songtitle <- songlistdf[isong,"title"]
      artistname <- songlistdf[isong,"artist"]
      genre <- songlistdf[isong,"genre"]
      totaldownloads <- songlistdf[isong,"count"]
      songdatafilename <- datafilename[isong]
      
      ## FIXME: used for debugging
      #print(songtitle)
            
      ## set timescale to aggregate at
      timescale <- timescale.save[isong]
      
      ## create pdfs
      pdf(sprintf("Top1000GBSongsPlots/%s%s.pdf",songdatafilename,country))
      
      ## load dataframe for download counts of song we want to examine
      ## and set appropriate names for aggfun1() to handle
      song1 <- read.delim(sprintf("../Top1000GBSongstsv/%s%s.tsv",songdatafilename,country)) %>%
        setNames(c("count","datetime")) # note we're calling this datetime so that aggfun1 can find it
      
      ## hesitantly adding this here as well
      song1$datetime <- as.POSIXct(as.character(song1$datetime),tz="GMT")
      
      ## aggregate song at the best timescale
      if (is.element(timescale,ymd.timescale)) {
        song <- aggfun1(song1,timescale,ymd)
        
        ## plot resulting curve
        titlestring <- paste(sprintf("Downloads in %s of '%s' ", country, songtitle), "\n", sprintf("by %s (%s)", artistname, genre), "\n", sprintf("(%s intervals)",timescale))
        
        ## Plot aggregated downloads
        with(song,plot(date,count,type="l",main=titlestring,xlim=c(min(date),max(date)),xlab="Date",ylab="Downloads",col="grey"))
        with(song,points(date,count,pch=21,bg="black",cex=0.75))
        
        ## load fit of data
        m1 <- song.fits[[isong]]
          
        ## make date numeric so that it is in the proper format to be used by ode() in SIR.detsim() below
        songdate.num <- with(song,as.numeric(date))
        song <- data.frame(song$count,songdate.num)
        names(song) <- c("count","date")
        
        ## plot time series with fit based on initial growth and SIR fit
        fit.parms <- coef(m1)
        names(fit.parms)[4] <- "logit.i0"
        tp <- trans.pars(fit.parms) # extracted parameters from sir fit
        ss <- SIR.detsim(seq_along(song$date),tp) # a deterministic SIR simulation using tp for parameters and a sequence of times adjusted to be the same as that of{
        ## the download data timescale
        with(song,lines(date,ss,col="forestgreen",lwd=3))
        
        ## add a legend - this option is being commented out for now in favour of adding parameters later without making the plots too busy{
        ##legend("topright",legend=c("Song Downloads","Fitted Model"),col=c("grey","forestgreen"),lwd=c(1,3),lty=rep(1,2),pch=c(NA,NA))
        ##legend("topright",legend=c("Song Downloads","Fitted Model"),col=c("black","black"),lty=rep(0,2),pch=c(21,NA),pt.bg=c("black",NA),bg=NA,bty="n")
        
        ## we will soon want to calculate proportional sum of squares as a measure of how `good' the fitted model is
        cc <- song$count
        
        
      } else {
        
        if (is.element(timescale,hms.timescale)) { # in this case we have to reaggregate the beginning... likely not the best way to do this
          song <- aggfun1(song1,breaks="day",back.conv=ymd)
          
          ## check where the maximum point falls in the download time series
          max.point <- which.max(song$count)
          ## make note of the date at which this max.point occurs
          peak.date <- as.Date(song$date[max.point]) # must coerce to a date, otherwise
                                        # it's a factor or something and doesn't work
          
          ## use this date to create a data frame containing downloads up to the
          ## peak point and a data frame containing downloads after this point
          song1beginning <- subset(song1,round_date(as.Date(song1$date),unit="day")<=peak.date)
          ## for song1beginning, song1$date was coerced to be a date rather than a list of factors (I
          ## think) and then rounded to be only days so that it could be compared to the date at
          ## which the peak number of downloads occurred
          song1end <- subset(song1,round_date(as.Date(song1$date),unit="day")>peak.date)
          song2end <- aggfun1(song1end,breaks="day",back.conv=ymd)
                    
          ## now better aggregate beginning of song only
          
          ## now aggregate at progressively smaller timescales the same as was
          ## done in my PlotAndParms scripts by checking the max.point each time
          ## create a finer timescale vector and a counter itime
          timescale.vec <- c("12 hour","6 hour","3 hour","1 hour","30 min","15 min")
          itime <- 1
          ## add this in case we don't enter the while loop
          song2beginning <- song1beginning %>%
            setNames(c("count","date"))
          ## from looking at some time series with missing beginnings, we will set the number of points
          ## required before the max.point to be higher now
          while (max.point<25 && itime<7) {
            timescale <- timescale.vec[itime]
            ## aggregate song download data
            song2beginning <- aggfun1(song1beginning,breaks=timescale,back.conv=ymd_hms) #note that now back.conv has to be ymd_hms since timescale is <1 day
            ## check where max downloads occurs
            max.point <- which.max(song2beginning$count)
            ## update counter
            itime <- itime+1
          }
                    
          ## put the more finely aggregated beginning of the time series back
          ## together with the rest of the time series
          song2 <- rbind(song2beginning,song2end)
          
          ## plot resulting curve
          titlestring <- paste(sprintf("Downloads in %s of '%s' ", country, songtitle), "\n", sprintf("by %s (%s)", artistname, genre), "\n", sprintf("(%s intervals for beg.)",timescale))
          
          ## Plot aggregated downloads
          with(song2,plot(date,count,type="l",main=titlestring,xlim=c(min(date),max(date)),xlab="Date",ylab="Downloads",col="grey"))
          with(song2,points(date,count,pch=21,bg="black",cex=0.75))
          
          ## load fit of data
          m1 <- song.fits[[isong]]
          
          ## make date numeric so that it is in the proper format to be used by ode() in SIR.detsim() below
          songdate.num <- with(song2,as.numeric(date))
          ## BMB: possibly confusing to replace song2 in this way
          song2 <- data.frame(count=song2$count,date=songdate.num)
          names(song2) <- c("count","date")
          
          ## plot time series with fit based on initial growth and SIR fit
          fit.parms <- coef(m1)
          names(fit.parms)[4] <- "logit.i0"
          tp <- trans.pars(fit.parms) # extracted parameters from sir fit
          tvec <- as.numeric(as.POSIXct(song2$date,origin="1970-1-1"))/86400 ## added Aug. 3
          ss <- SIR.detsim(tvec,tp) # a deterministic SIR simulation using tp for parameters and a sequence of times adjusted to be the same as that of
          ## the download data timescale
          with(song2,lines(as.POSIXct(date,origin="1970-1-1"),
                             ss,col="forestgreen",lwd=3))
          
          ## add a legend - this option is being commented out for now in favour of adding parameters later without making the plots too busy
          ##legend("topright",legend=c("Song Downloads","Fitted Model"),col=c("grey","forestgreen"),lwd=c(1,3),lty=rep(1,2),pch=c(NA,NA))
          ##legend("topright",legend=c("Song Downloads","Fitted Model"),col=c("black","black"),lty=rep(0,2),pch=c(21,NA),pt.bg=c("black",NA),bg=NA,bty="n")
          
          ## we will soon want to calculate proportional sum of squares as a measure of how `good' the fitted model is
          cc <- song2$count
          
          
        } else {
          warning("Timescale error")
        }
      }
      
      ##plot(song$date,song$count) # if we were to plot this, we would use the sequence of times from the date
      ##lines(song$date,ss,col=14) # vector in song data and the deterministic SIR simulation that was made with
                                        # the adjusted timescale (I did test to make sure this plotted the appropriate
                                        # line for the fit)

      ## calculate how `good' this fit is, then save it in a vector with everything else
      ## first relative
      rel.sum.sq <- mean((1-ss/cc)^2)
      ## then root of this
      root.mean.sq <- sqrt(rel.sum.sq)
      ## also absolute
      abs.sum.sq <- mean((cc-ss)^2)
      
      ## store appropriate SIR parameters in each vector
      R0.vec[isong] <- summarize.pars(fit.parms)[1]
      gamma.vec[isong] <- trans.pars(fit.parms)[2]
      beta.vec[isong] <- trans.pars(fit.parms)[1]
      infper.vec[isong] <- summarize.pars(fit.parms)[3]
      r.vec[isong] <- summarize.pars(fit.parms)[2]
      N.vec[isong] <- trans.pars(fit.parms)[3]
      s0.vec[isong] <- trans.pars(fit.parms)[4]
      i0.vec[isong] <- summarize.pars(fit.parms)[4]
      rel.fit.goodness.vec[isong] <- root.mean.sq
      abs.fit.goodness.vec[isong] <- abs.sum.sq
      
      ## calculate final size
      z.vec[isong] <- Z(R0.vec[isong])
      ## use this to calculate a number for initial number of susceptibles
      s0calc.vec[isong] <- totaldownloads/z.vec[isong]
      
      ## add parameters to the plot as a legend
      songR0 <- summarize.pars(fit.parms)[1]
      songinfper <- summarize.pars(fit.parms)[3]
      relsongfit <- root.mean.sq
      abssongfit <- abs.sum.sq
      parmstring <- sprintf("R_0 = %.3f\nInfectious period = %.3f days\nRelative Fit Measure = %.3f\nAbsolute Fit Measure = %3f", songR0, songinfper,relsongfit,abssongfit)
      
      legend("topright",legend=parmstring,bty="n")
      
      ## The below is only implemented if we are examining fit of the curve at at different chunks of a song's download curve
      if (do.chunks) {
        if (songR0>=min.R0 & relsongfit<max.rel.fit & !is.element(songtitle,songstoexcludedf$title)) { # i.e. if it meets our good fit criteria and isn't a Christmas song
          
          print(songtitle)#dolphin remove
          b <- b+1
          print(b)
          ## now split time series into four chunks to check how SIR fits in each chunk
          ## another way to identify time of peak, which doesn't work for some songs that don't increase but just peak immediately:
          ##deriv1 <- sign(diff(ss))
          ##t_peak <- which(diff(deriv1)==-2)
          t_peak <- which.max(ss)
          
          cc.sum <- cumsum(cc) ## cumulative sum of downloads
          cc.sum.peak <- cc.sum[t_peak]
          if (t_peak!=1 & length(which(cc.sum<=cc.sum.peak/2))>0) { ## also check that there are enough points before the peak to split this bit in half
            cut1 <- which(cc.sum<=cc.sum.peak/2)[length(which(cc.sum<=cc.sum.peak/2))] ## last index that is less than or equal to half the sum at the peak
            if (is.element(timescale,ymd.timescale)) {
              cut1.vec[isong] <- song$date[cut1]
            } else {
              cut1.vec[isong] <- song2$date[cut1]
            }
          } ## if t_peak!=1 & length(which(cc.sum<=cc.sum.peak/2))>0
          cut2 <- t_peak ## the second cut occurs at the peak
          cc.sum.2nd <- cc.sum[length(cc.sum)]-cc.sum.peak ## cumulative sum of second half of curve
          cc.sum.cut3 <- cc.sum.peak+cc.sum.2nd/2 ## cumulative sum up to third cut
          cut3 <- which(cc.sum>=cc.sum.cut3)[1] ## third cut
          if (is.element(timescale,ymd.timescale)) {
            cut2.vec[isong] <- song$date[cut2]
            cut3.vec[isong] <- song$date[cut3]
          } else {
            cut2.vec[isong] <- song2$date[cut2]
            cut3.vec[isong] <- song2$date[cut3]
          }
          ## cut cc and ss into two or four parts with these indices, depending on if there's anything before the peak
          if (t_peak==1) {
            cc.chunk3 <- cc[1:cut3]
            ss.chunk3 <- ss[1:cut3]
            cc.chunk4 <- cc[(cut3+1):length(cc)]
            ss.chunk4 <- ss[(cut3+1):length(ss)]
            stopifnot(length(c(cc.chunk3,cc.chunk4))==length(cc)) ## check this worked properly
            ## calculate goodness of fit for each chunk
            chunk3.rel.fit <- mean((1-ss.chunk3/cc.chunk3)^2)
            chunk4.rel.fit <- mean((1-ss.chunk4/cc.chunk4)^2)
            ## and store the values
            chunk3.rel.fit.vec[isong] <- chunk3.rel.fit
            chunk4.rel.fit.vec[isong] <- chunk4.rel.fit
            ## now normalized cumulative sum at end of each chunk
            ccs <- cumsum(cc)/sum(cc)
            ccs3 <- ccs[cut3] ## normalized cumsum after first cut
            ## store this
            ccs3.vec[isong] <- ccs3
            
            ## plot vertical lines showing where we cut time series into chunks
            ## to further examine goodness of fit
            if (plot.chunks) {
              if (is.element(timescale,ymd.timescale)) {
                abline(v=song$date[cut2],col=2)
                abline(v=song$date[cut3],col=2)
              } else {
                if (is.element(timescale,hms.timescale)) {
                  abline(v=song2$date[cut2],col=2)
                  abline(v=song2$date[cut3],col=2)
                } else {
                  warning("Still a timescale error!")
                }
              } ## end of timescale checking for plotting vertical chunk lines              
            } ## if plot.chunks
            
          } else { ##if t_peak==1
            
            if (length(which(cc.sum<=cc.sum.peak/2))>0) { ## if there are enough points before the peak to cut this bit in half
              cc.chunk1 <- cc[1:cut1] ## going up to cut1 in case it is index 1 -- had issues with this previously
              ss.chunk1 <- ss[1:cut1]
              cc.chunk2 <- cc[(cut1+1):cut2]
              ss.chunk2 <- ss[(cut1+1):cut2]
              cc.chunk3 <- cc[(cut2+1):cut3]
              ss.chunk3 <- ss[(cut2+1):cut3]
              cc.chunk4 <- cc[(cut3+1):length(cc)]
              ss.chunk4 <- ss[(cut3+1):length(ss)]
              stopifnot(length(c(cc.chunk1,cc.chunk2,cc.chunk3,cc.chunk4))==length(cc)) ## check this worked properly
              ## calculate goodness of fit for each chunk
              chunk1.rel.fit <- mean((1-ss.chunk1/cc.chunk1)^2)
              chunk2.rel.fit <- mean((1-ss.chunk2/cc.chunk2)^2)
              chunk3.rel.fit <- mean((1-ss.chunk3/cc.chunk3)^2)
              chunk4.rel.fit <- mean((1-ss.chunk4/cc.chunk4)^2)
              ## and store the values
              chunk1.rel.fit.vec[isong] <- chunk1.rel.fit
              chunk2.rel.fit.vec[isong] <- chunk2.rel.fit
              chunk3.rel.fit.vec[isong] <- chunk3.rel.fit
              chunk4.rel.fit.vec[isong] <- chunk4.rel.fit
              ## now normalized cumulative sum at end of each chunk
              ccs <- cumsum(cc)/sum(cc)
              ccs1 <- ccs[cut1] ## normalized cumsum after first cut
              ccs2 <- ccs[cut2]
              ccs3 <- ccs[cut3]
              ## store this
              ccs1.vec[isong] <- ccs1
              ccs2.vec[isong] <- ccs2
              ccs3.vec[isong] <- ccs3
              if (pre.peak.fit) { ## if we want to look at the goodness of fit for the entire bit before the peak
                cc.pre.peak <- cc[1:cut2]
                ss.pre.peak <- ss[1:cut2]
                pre.peak.rel.fit <- mean((1-ss.pre.peak/cc.pre.peak)^2)
                pre.peak.rel.fit.vec[isong] <- pre.peak.rel.fit#dolphin
              } ## if pre.peak.fit
              
              ## plot vertical lines showing where we cut time series into chunks
              ## to further examine goodness of fit
              if (plot.chunks) {
                if (is.element(timescale,ymd.timescale)) {
                  abline(v=song$date[cut1],col=2)
                  abline(v=song$date[cut2],col=2)
                  abline(v=song$date[cut3],col=2)
                } else {
                  if (is.element(timescale,hms.timescale)) {
                    abline(v=song2$date[cut1],col=2)
                    abline(v=song2$date[cut2],col=2)
                    abline(v=song2$date[cut3],col=2)
                  } else {
                    warning("Still a timescale error!")
                  }
                } ## end of timescale checking for plotting vertical chunk lines              
              } ## if plot.chunks
              
            } else { ## end of if (length(which(cc.sum<=cc.sum.peak/2))>0) i.e., if t_peak!=1 but there is not enough before the peak to split in two
              cc.chunk3 <- cc[(cut2+1):cut3]
              ss.chunk3 <- ss[(cut2+1):cut3]
              cc.chunk4 <- cc[(cut3+1):length(cc)]
              ss.chunk4 <- ss[(cut3+1):length(ss)]
              stopifnot(length(c(cc[1:cut2],cc.chunk3,cc.chunk4))==length(cc)) ## check this worked properly
              ## calculate goodness of fit for each chunk
              chunk3.rel.fit <- mean((1-ss.chunk3/cc.chunk3)^2)
              chunk4.rel.fit <- mean((1-ss.chunk4/cc.chunk4)^2)
              ## and store the values
              chunk3.rel.fit.vec[isong] <- chunk3.rel.fit
              chunk4.rel.fit.vec[isong] <- chunk4.rel.fit
              ## now normalized cumulative sum at end of each chunk
              ccs <- cumsum(cc)/sum(cc)
              ccs2 <- ccs[cut2] ## normalized cumsum after first cut
              ccs3 <- ccs[cut3]
              ## store this
              ccs2.vec[isong] <- ccs2
              ccs3.vec[isong] <- ccs3
              if (pre.peak.fit) { ## can still do this if t_peak!=1
                cc.pre.peak <- cc[1:cut2]
                ss.pre.peak <- ss[1:cut2]
                pre.peak.rel.fit <- mean((1-ss.pre.peak/cc.pre.peak)^2)
                pre.peak.rel.fit.vec[isong] <- pre.peak.rel.fit#dolphin
              } ## if pre.peak.fit
              
              ## plot vertical lines showing where we cut time series into chunks
              ## to further examine goodness of fit
              if (plot.chunks) {
                if (is.element(timescale,ymd.timescale)) {
                  abline(v=song$date[cut2],col=2)
                  abline(v=song$date[cut3],col=2)
                } else {
                  if (is.element(timescale,hms.timescale)) {
                    abline(v=song2$date[cut2],col=2)
                    abline(v=song2$date[cut3],col=2)
                  } else {
                    warning("Still a timescale error!")
                  }
                } ## end of timescale checking for plotting vertical chunk lines              
              } ## if plot.chunks
              
            } ## else, i.e., if t_peak is not 1 and there is not enough to cut the bit before the peak in half
            
          } ## else, i.e. if t_peak is not 1
          
        } ##if songR0>=min.R0, etc.
      } ##if do.chunks
      
      ## done with figures
      dev.off()
      
    }
    
    ## add parameter data to song dataframe
    if (pre.peak.fit) {
      if (do.chunks) {
        songlistdf <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec,chunk1.rel.fit.vec,chunk2.rel.fit.vec,chunk3.rel.fit.vec,chunk4.rel.fit.vec,pre.peak.rel.fit.vec,ccs1.vec,ccs2.vec,ccs3.vec)
        names(songlistdf)[5:24] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness","chunk1.rel.fit","chunk2.rel.fit","chunk3.rel.fit","chunk4.rel.fit","pre.peak.fit","cumsum1","cumsum2","cumsum3")
        ## save a separate data frame that also includes the times of each cut for the chunks -- this one will only be saved in the RData file
        songlistdf2 <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec,chunk1.rel.fit.vec,chunk2.rel.fit.vec,chunk3.rel.fit.vec,chunk4.rel.fit.vec,pre.peak.rel.fit.vec,ccs1.vec,ccs2.vec,ccs3.vec,cut1.vec,cut2.vec,cut3.vec)
        names(songlistdf2)[5:27] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness","chunk1.rel.fit","chunk2.rel.fit","chunk3.rel.fit","chunk4.rel.fit","pre.peak.fit","cumsum1","cumsum2","cumsum3","cut1.vec","cut2.vec","cut3.vec")
      } else { ##if do.chunks
        songlistdf <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec)
        names(songlistdf)[5:16] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness")
      } ##else for do.chunks
    } else { ##if pre.peak.fit
      if (do.chunks) {
        songlistdf <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec,chunk1.rel.fit.vec,chunk2.rel.fit.vec,chunk3.rel.fit.vec,chunk4.rel.fit.vec,ccs1.vec,ccs2.vec,ccs3.vec)
        names(songlistdf)[5:23] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness","chunk1.rel.fit","chunk2.rel.fit","chunk3.rel.fit","chunk4.rel.fit","cumsum1","cumsum2","cumsum3")
        ## save a separate data frame that also includes the times of each cut for the chunks -- this one will only be saved in the RData file
        songlistdf2 <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec,chunk1.rel.fit.vec,chunk2.rel.fit.vec,chunk3.rel.fit.vec,chunk4.rel.fit.vec,ccs1.vec,ccs2.vec,ccs3.vec,cut1.vec,cut2.vec,cut3.vec)
        names(songlistdf2)[5:26] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness","chunk1.rel.fit","chunk2.rel.fit","chunk3.rel.fit","chunk4.rel.fit","cumsum1","cumsum2","cumsum3","cut1.vec","cut2.vec","cut3.vec")
      } else { ##if do.chunks
        songlistdf <- cbind(songlistdf[,1:4],R0.vec,r.vec,infper.vec,i0.vec,beta.vec,gamma.vec,N.vec,s0.vec,s0calc.vec,z.vec,rel.fit.goodness.vec,abs.fit.goodness.vec)
        names(songlistdf)[5:16] <- c("R0","r","infper","i0","beta","gamma","N","Extracted s0","Calculated s0","Z","rel.fit.goodness","abs.fit.goodness")
      } ##else for do.chunks
    } ##else for pre.peak.fit
    
    filename <- paste(sprintf("songlistparams%s.tsv",country))
    
    ## store these parameters in a file
    ## in this case there is one file for each country
    write.table(songlistdf,file=filename,row.names=FALSE,quote=FALSE,sep="\t",append=TRUE)
    
    ## save song parameters to an .RData file now
    filename <- paste(sprintf("TopSongs%sPars_Accents.RData",country))
    if (do.chunks) {
      save(songlistdf2,file=filename)
    } else {
      save(songlistdf,file=filename)
    }
  }
}


warnings()
